A Layered Protocol Architecture for the Internet of Agents

Charles Fleming
Cisco Research
USA

Vijoy Pandey
Cisco Research
USA

Luca Muscariello
Cisco Research
France

Ramana Kompella
Cisco Research
USA

5
2
0
2

v
o
N
6
2

]
I

N
.
s
c
[

2
v
9
9
6
9
1
.
1
1
5
2
:
v
i
X
r
a

Abstract
Large Language Models (LLMs) have demonstrated remarkable per-
formance improvements and the ability to learn domain-specific
languages (DSLs), including APIs and tool interfaces. This capability
has enabled the creation of AI agents that can perform preliminary
computations and act through tool calling, now being standardized
via protocols like MCP. However, LLMs face fundamental limita-
tions: their context windows cannot grow indefinitely, constraining
their memory and computational capacity. Agent collaboration
emerges as essential for solving increasingly complex problems,
mirroring how computational systems rely on different types of
memory to scale. The "Internet of Agents" (IoA) represents the
communication stack that enables agents to scale by distributing
computation across collaborating entities.

Current network architectural stacks (OSI and TCP/IP) were de-
signed for data delivery between hosts and processes, not for agent
collaboration with semantic understanding. To address this gap, we
propose two new layers: an Agent Communication Layer (L8)
and an Agent Semantic Negotiation Layer (L9). L8 formalizes
the structure of communication, standardizing message envelopes,
speech-act performatives (e.g., REQUEST, INFORM), and interac-
tion patterns (e.g., request-reply, publish-subscribe), building on
protocols like MCP. L9, which does not exist today, formalizes the
meaning of communication, enabling agents to discover, negoti-
ate, and lock a "Shared Context"—a formal schema defining the
concepts, tasks, and parameters relevant to their interaction. To-
gether, these layers provide the foundation for scalable, distributed
agent collaboration, enabling the next generation of multi-agentic
systems.

1 Introduction
Large Language Models (LLMs) have demonstrated remarkable
advances in performance, fundamentally transforming how we
approach complex computational tasks. A key breakthrough has
been their ability to learn and work with domain-specific languages
(DSLs), including APIs, tool interfaces, and structured data formats.
This capability has enabled the creation of AI agents that go be-
yond simple text generation to perform preliminary computations,
execute actions, and interact with external systems through stan-
dardized protocols like the Model Context Protocol (MCP) [5].

However, LLMs face fundamental architectural constraints. Their
context windows, while expanding, cannot grow indefinitely, lim-
iting their working memory and the complexity of problems they
can solve independently. This limitation mirrors challenges in tra-
ditional computing systems, where single processors hit physical
and thermal constraints. The solution in both cases is the same:
distribution and collaboration. Just as computational systems scale
through distributed architectures with various memory hierarchies,
AI agents must collaborate to tackle problems beyond individual
capacity [2].

The analogy to distributed computing runs deep. Distributed
computing enables the implementation of distributed algorithms us-
ing specialized programming languages, with parallel programming
representing the simplest form—replication of identical computa-
tional tasks across shards. More sophisticated distributed program-
ming requires decomposing algorithms into specialized compu-
tational tasks, a fundamentally harder problem. While not every
problem can be optimally distributed, heuristics and approxima-
tions often scale favorably, trading perfect solutions for tractable
ones at larger problem sizes. Multi-agent systems represent a re-
alization of distributed computation with a compelling feature:
autonomous action. LLM-based multi-agent systems add another
dimension—the ability to work across multiple languages, both
natural and domain-specific.

Programming languages are themselves domain-specific lan-
guages, and compilers serve as DSL validators of both syntax and
semantics within the context of computation. LLMs excel at translat-
ing between languages—including from natural language to DSLs—
but